[{"authors":["admin"],"categories":null,"content":"I am Ph.D. Candidate in Shanghai Jiao Tong University, under the supervision of Kai Yu and Yanmin Qian. My research interests include deep learning based approaches for speaker recognition, speaker diarization and voice activity detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://wsstriving.github.io/author/shuai-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuai-wang/","section":"authors","summary":"I am Ph.D. Candidate in Shanghai Jiao Tong University, under the supervision of Kai Yu and Yanmin Qian. My research interests include deep learning based approaches for speaker recognition, speaker diarization and voice activity detection.","tags":null,"title":"Shuai Wang","type":"authors"},{"authors":["Federico Landini","Shuai Wang","Mireia Diez","Lukáš Burget","Pavel Matějka","Kateřina Žmolíková","Ladislav Mošner","Anna Silnova","Oldřich Plchot","Ondřej Novotný","Hossein Zeinali","Johan Rohdin"],"categories":["diarization"],"content":"","date":1593092762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092762,"objectID":"236062526873f7594d03148245c4040b","permalink":"https://wsstriving.github.io/publication/2020-icassp-dihard/","publishdate":"2020-06-25T21:46:02+08:00","relpermalink":"/publication/2020-icassp-dihard/","section":"publication","summary":"This paper describes the winning systems developed by the BUT team for the four tracks of the Second DIHARD Speech Diarization Challenge. For tracks 1 and 2 the systems were mainly based on performing agglomerative hierarchical clustering (AHC) of x-vectors, followed by another x-vector clustering based on Bayes hidden Markov model and variational Bayes inference. We provide a comparison of the improvement given by each step and share the implementation of the core of the system. For tracks 3 and 4 with recordings from the Fifth CHiME Challenge, we explored different approaches for doing multi-channel diarization and our best performance was obtained when applying AHC on the fusion of per channel probabilistic linear discriminant analysis scores.","tags":[],"title":"But System for the Second Dihard Speech Diarization Challenge","type":"publication"},{"authors":["Mireia Diez","Lukáš Burget","Federico Landini","Shuai Wang","Honza Černocký"],"categories":["diarization"],"content":"","date":1593092757,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092757,"objectID":"e731bd48fbdbc402ae4c4569bca9bfc6","permalink":"https://wsstriving.github.io/publication/2020-icassp-vb/","publishdate":"2020-06-25T21:45:57+08:00","relpermalink":"/publication/2020-icassp-vb/","section":"publication","summary":"This paper presents an analysis of our diarization system winning the second DIHARD speech diarization challenge, track 1. This system is based on clustering x-vector speaker embeddings extracted every 0.25s from short segments of the input recording. In this paper, we focus on the two x-vector clustering methods employed, namely Agglomerative Hierarchical Clustering followed by a clustering based on Bayesian Hidden Markov Model (BHMM). Even though the system submitted to the challenge had further post-processing steps, we will show that using this BHMM solely is enough to achieve the best performance in the challenge. The analysis will show improvements achieved by optimizing individual processing steps, including a simple procedure to effectively perform domain adaptation by Probabilistic Linear Discriminant Analysis model interpolation. All experiments are performed in the DIHARD II evaluation framework.","tags":[],"title":"Optimizing Bayesian HMM based x-vector clustering for the second DIHARD speech diarization challenge","type":"publication"},{"authors":["Yexin Yang","Shuai Wang","Xun Gong","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1593092752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092752,"objectID":"31049aaa0b60059d482c4bd57ff85269","permalink":"https://wsstriving.github.io/publication/2020-icassp-adapt/","publishdate":"2020-06-25T21:45:52+08:00","relpermalink":"/publication/2020-icassp-adapt/","section":"publication","summary":"Text mismatch between pre-collected data, either training data or enrollment data, and the actual test data can significantly hurt text-dependent speaker verification (SV) system performance. Although this problem can be solved by carefully collecting data with the target speech content, such data collection could be costly and inflexible. In this paper, we propose a novel text adaptation framework to address the text mismatch issue. Here, a speaker-text factorization network is proposed to factorize the input speech into speaker embeddings and text embeddings and then integrate them into a single representation in the later stage. Given a small amount of speaker-independent adaptation utterances, text embeddings of target speech content can be extracted and used to adapt the text-independent speaker embeddings to text-customized speaker embeddings. Experiments on RSR2015 show that text adaptation can significantly improve the performance of text mismatch conditions","tags":[],"title":"Text Adaptation for Speaker Verification with Speaker-Text Factorized Embeddings","type":"publication"},{"authors":["Zhengyang Chen","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["sid"],"content":"","date":1593092745,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092745,"objectID":"233e9b343cd1a58ae192670662893b4f","permalink":"https://wsstriving.github.io/publication/2020-icassp-channel/","publishdate":"2020-06-25T21:45:45+08:00","relpermalink":"/publication/2020-icassp-channel/","section":"publication","summary":"Using deep neural network to extract speaker embedding has significantly improved the speaker verification task. However, such embeddings are still vulnerable to channel variability. Previous works have used adversarial training to suppress channel information to extract channel-invariant embedding and achieved a significant improvement. Inspired by the successful joint multi-task and adversarial training with phonetic information for phonetic-invariant speaker embedding learning, in this paper, a similar methodology is developed to suppress the channel variability. By treating the recording devices or environments as the channel variability, two individual experiments are carried out, and consistent performance improvement is observed in both cases. The best performance is obtained by sequentially applying multi-task training at the statistics pooling layer and adversarial training at the embedding layer, achieving 10.77% and 9.37% relative improvements in terms of EER compared to the baselines, for the recording environments or devices level, respectively","tags":["robustness"],"title":"Channel Invariant Speaker Embedding Learning with Joint Multi-Task and Adversarial Training","type":"publication"},{"authors":["Shuai Wang","Johan Rohdin","Oldřich Plchot","Lukáš Burget","Kai Yu","Jan Černocký"],"categories":["sid"],"content":"","date":1593092736,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092736,"objectID":"dc85fba0891293c64389195f152a5fa2","permalink":"https://wsstriving.github.io/publication/2020-icassp-specaug/","publishdate":"2020-06-25T21:45:36+08:00","relpermalink":"/publication/2020-icassp-specaug/","section":"publication","summary":"SpecAugment is a newly proposed data augmentation method for speech recognition. By randomly masking bands in the log Mel spectogram this method leads to impressive performance improvements. In this paper, we investigate the usage of SpecAugment for speaker verification tasks. Two different models, namely 1-D convolutional TDNN and 2-D convolutional ResNet34, trained with either Softmax or AAM-Softmax loss, are used to analyze SpecAugment’s effectiveness. Experiments are carried out on the Voxceleb and NIST SRE 2016 dataset. By applying SpecAugment to the original clean data in an on-the-fly manner without complex off-line data augmentation methods, we obtained 3.72% and 11.49% EER for NIST SRE 2016 Cantonese and Tagalog, respectively. For Voxceleb1 evaluation set, we obtained 1.47% EER.","tags":[],"title":"Investigation of Specaugment for Deep Speaker Embedding Learning","type":"publication"},{"authors":["Shuai Wang","Yanmin Qian and Kai Yu"],"categories":[],"content":"","date":1498384882,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498384882,"objectID":"49daf81e9b3979471a1960974d3c31d0","permalink":"https://wsstriving.github.io/publication/2017-interspeech/","publishdate":"2020-06-25T18:01:22+08:00","relpermalink":"/publication/2017-interspeech/","section":"publication","summary":"summary","tags":[],"title":"What Does the Speaker Embedding Encode?","type":"publication"}]