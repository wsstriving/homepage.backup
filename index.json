[{"authors":["admin"],"categories":null,"content":"I am Ph.D. Candidate in Shanghai Jiao Tong University, under the supervision of Kai Yu and Yanmin Qian. My research interests include deep learning based approaches for speaker recognition, speaker diarization and voice activity detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://wsstriving.github.io/author/shuai-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/shuai-wang/","section":"authors","summary":"I am Ph.D. Candidate in Shanghai Jiao Tong University, under the supervision of Kai Yu and Yanmin Qian. My research interests include deep learning based approaches for speaker recognition, speaker diarization and voice activity detection.","tags":null,"title":"Shuai Wang","type":"authors"},{"authors":["Federico Landini","Shuai Wang","Mireia Diez","Lukáš Burget","Pavel Matějka","Kateřina Žmolíková","Ladislav Mošner","Anna Silnova","Oldřich Plchot","Ondřej Novotný","Hossein Zeinali","Johan Rohdin"],"categories":["diarization"],"content":"","date":1593092762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092762,"objectID":"236062526873f7594d03148245c4040b","permalink":"https://wsstriving.github.io/publication/2020-icassp-dihard/","publishdate":"2020-06-25T21:46:02+08:00","relpermalink":"/publication/2020-icassp-dihard/","section":"publication","summary":"This paper describes the winning systems developed by the BUT team for the four tracks of the Second DIHARD Speech Diarization Challenge. For tracks 1 and 2 the systems were mainly based on performing agglomerative hierarchical clustering (AHC) of x-vectors, followed by another x-vector clustering based on Bayes hidden Markov model and variational Bayes inference. We provide a comparison of the improvement given by each step and share the implementation of the core of the system. For tracks 3 and 4 with recordings from the Fifth CHiME Challenge, we explored different approaches for doing multi-channel diarization and our best performance was obtained when applying AHC on the fusion of per channel probabilistic linear discriminant analysis scores.","tags":[],"title":"But System for the Second Dihard Speech Diarization Challenge","type":"publication"},{"authors":["Mireia Diez","Lukáš Burget","Federico Landini","Shuai Wang","Honza Černocký"],"categories":["diarization"],"content":"","date":1593092757,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092757,"objectID":"e731bd48fbdbc402ae4c4569bca9bfc6","permalink":"https://wsstriving.github.io/publication/2020-icassp-vb/","publishdate":"2020-06-25T21:45:57+08:00","relpermalink":"/publication/2020-icassp-vb/","section":"publication","summary":"This paper presents an analysis of our diarization system winning the second DIHARD speech diarization challenge, track 1. This system is based on clustering x-vector speaker embeddings extracted every 0.25s from short segments of the input recording. In this paper, we focus on the two x-vector clustering methods employed, namely Agglomerative Hierarchical Clustering followed by a clustering based on Bayesian Hidden Markov Model (BHMM). Even though the system submitted to the challenge had further post-processing steps, we will show that using this BHMM solely is enough to achieve the best performance in the challenge. The analysis will show improvements achieved by optimizing individual processing steps, including a simple procedure to effectively perform domain adaptation by Probabilistic Linear Discriminant Analysis model interpolation. All experiments are performed in the DIHARD II evaluation framework.","tags":[],"title":"Optimizing Bayesian HMM based x-vector clustering for the second DIHARD speech diarization challenge","type":"publication"},{"authors":["Yexin Yang","Shuai Wang","Xun Gong","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1593092752,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092752,"objectID":"31049aaa0b60059d482c4bd57ff85269","permalink":"https://wsstriving.github.io/publication/2020-icassp-adapt/","publishdate":"2020-06-25T21:45:52+08:00","relpermalink":"/publication/2020-icassp-adapt/","section":"publication","summary":"We proposed the text-adptation speaker verification task and an intital solution called Speaker-text factorization network, which could deal with different text-mismatch conditions","tags":[],"title":"Text Adaptation for Speaker Verification with Speaker-Text Factorized Embeddings","type":"publication"},{"authors":["Zhengyang Chen","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1593092745,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092745,"objectID":"233e9b343cd1a58ae192670662893b4f","permalink":"https://wsstriving.github.io/publication/2020-icassp-channel/","publishdate":"2020-06-25T21:45:45+08:00","relpermalink":"/publication/2020-icassp-channel/","section":"publication","summary":"Using deep neural network to extract speaker embedding has significantly improved the speaker verification task. However, such embeddings are still vulnerable to channel variability. Previous works have used adversarial training to suppress channel information to extract channel-invariant embedding and achieved a significant improvement. Inspired by the successful joint multi-task and adversarial training with phonetic information for phonetic-invariant speaker embedding learning, in this paper, a similar methodology is developed to suppress the channel variability. By treating the recording devices or environments as the channel variability, two individual experiments are carried out, and consistent performance improvement is observed in both cases. The best performance is obtained by sequentially applying multi-task training at the statistics pooling layer and adversarial training at the embedding layer, achieving 10.77% and 9.37% relative improvements in terms of EER compared to the baselines, for the recording environments or devices level, respectively","tags":[],"title":"Channel Invariant Speaker Embedding Learning with Joint Multi-Task and Adversarial Training","type":"publication"},{"authors":["Shuai Wang","Johan Rohdin","Oldřich Plchot","Lukáš Burget","Kai Yu","Jan Černocký"],"categories":["sid"],"content":"","date":1593092736,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593092736,"objectID":"dc85fba0891293c64389195f152a5fa2","permalink":"https://wsstriving.github.io/publication/2020-icassp-specaug/","publishdate":"2020-06-25T21:45:36+08:00","relpermalink":"/publication/2020-icassp-specaug/","section":"publication","summary":"SpecAugment is a newly proposed data augmentation method for speech recognition. By randomly masking bands in the log Mel spectogram this method leads to impressive performance improvements. In this paper, we investigate the usage of SpecAugment for speaker verification tasks. Two different models, namely 1-D convolutional TDNN and 2-D convolutional ResNet34, trained with either Softmax or AAM-Softmax loss, are used to analyze SpecAugment’s effectiveness. Experiments are carried out on the Voxceleb and NIST SRE 2016 dataset. By applying SpecAugment to the original clean data in an on-the-fly manner without complex off-line data augmentation methods, we obtained 3.72% and 11.49% EER for NIST SRE 2016 Cantonese and Tagalog, respectively. For Voxceleb1 evaluation set, we obtained 1.47% EER.","tags":[],"title":"Investigation of Specaugment for Deep Speaker Embedding Learning","type":"publication"},{"authors":["Yexin Yang","Hongji Wang","Heinrich Dinkel","Zhengyang Chen","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["antispoof"],"content":"","date":1561473614,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473614,"objectID":"145c2679ec539a04780d22bca57b6213","permalink":"https://wsstriving.github.io/publication/2019-is-asvspoof/","publishdate":"2020-06-25T22:40:14+08:00","relpermalink":"/publication/2019-is-asvspoof/","section":"publication","summary":"The robustness of an anti-spoofing system is progressively more important in order to develop a reliable speaker verification system. Previous challenges and datasets mainly focus on a specific type of spoofing attacks. The ASVspoof 2019 edition is the first challenge to address two major spoofing types-logical and physical access. This paper presents the SJTU’s submitted antispoofing system to the ASVspoof 2019 challenge. Log-CQT features are developed in conjunction with multi-layer convolutional neural networks for robust performance across both subtasks. CNNs with gradient linear units (GLU) activations are utilized for spoofing detection. The proposed system shows consistent performance improvement over all types of spoofing attacks. Our primary submissions achieve the 5th and 8th positions for the logical and physical access respectively. Moreover, our contrastive submission to the PA task exhibits","tags":[],"title":"The SJTU Robust Anti-Spoofing System for the ASVspoof 2019 Challenge","type":"publication"},{"authors":["Mireia Diez","Lukáš Burget","Shuai Wang","Johan Rohdin","Jan Černocký"],"categories":["diarization"],"content":"","date":1561473597,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473597,"objectID":"94309524df07368a9a0d3b263cdfd082","permalink":"https://wsstriving.github.io/publication/2019-is-vb/","publishdate":"2020-06-25T22:39:57+08:00","relpermalink":"/publication/2019-is-vb/","section":"publication","summary":"This paper presents a simplified version of the previously proposed diarization algorithm based on Bayesian Hidden Markov Models, which uses Variational Bayesian inference for very fast and robust clustering of x-vector (neural network based speaker embeddings). The presented results show that this clustering algorithm provides significant improvements in diarization performance as compared to the previously used Agglomerative Hierarchical Clustering. The output of this system can be further employed as an initialization for a second stage VB diarization system, using frame-wise MFCC features as input, to obtain optimal results.","tags":[],"title":"Bayesian HMM Based x-Vector Clustering for Speaker Diarization","type":"publication"},{"authors":["Hongji Wang","Heinrich Dinkel","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["antispoof"],"content":"","date":1561473594,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473594,"objectID":"ffde9d6e4c496b23dd5f1313152cdf37","permalink":"https://wsstriving.github.io/publication/2019-is-spoof/","publishdate":"2020-06-25T22:39:54+08:00","relpermalink":"/publication/2019-is-spoof/","section":"publication","summary":"Replay spoofing attacks are a major threat for speaker verification systems. Although many anti-spoofing systems or countermeasures are proposed to detect dataset-specific replay attacks with promising performance, they generalize poorly when applied on unseen datasets. In this work, the cross-dataset scenario is treated as a domain-mismatch problem and dealt with using a domain adversarial training framework. Compared with previous approaches, features learned from this newly-designed architecture are more discriminative for spoofing detection, but more indistinguishable across different domains. Only labeled source-domain data and unlabeled target-domain data are required during the adversarial training process, which can be regarded as unsupervised domain adaptation. Experiments on the ASVspoof 2017 V. 2 dataset as well as the physical access condition part of BTAS 2016 dataset demonstrate that a significant EER reduction of over relative 30% can be obtained after applying the proposed domain adversarial training framework. It is shown that our proposed model can benefit from a large amount of unlabeled target-domain training data to improve detection accuracy.","tags":[],"title":"Cross-Domain Replay Spoofing Attack Detection Using Domain Adversarial Training","type":"publication"},{"authors":["Zhanghao Wu","Shuai Wang","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1561473566,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473566,"objectID":"cf315e8ccba60599b01533a44842ebc2","permalink":"https://wsstriving.github.io/publication/2019-is-vae/","publishdate":"2020-06-25T22:39:26+08:00","relpermalink":"/publication/2019-is-vae/","section":"publication","summary":"Domain or environment mismatch between training and testing, such as various noises and channels, is a major challenge for speaker verification. In this paper, a variational autoencoder (VAE) is designed to learn the patterns of speaker embeddings extracted from noisy speech segments, including i-vector and x-vector, and generate embeddings with more diversity to improve the robustness of speaker verification systems with probabilistic linear discriminant analysis (PLDA) back-end. The approach is evaluated on the standard NIST SRE 2016 dataset. Compared to manual and generative adversarial network (GAN) based augmentation approaches, the proposed VAE based augmentation achieves a slightly better performance for i-vector on Tagalog and Cantonese with EERs of 15.54% and 7.84%, and a more significant improvement for x-vector on those two languages with EERs of 11.86% and 4.20%.","tags":[],"title":"Data Augmentation Using Variational Autoencoder for Embedding Based Speaker Verification","type":"publication"},{"authors":["Shuai Wang","Johan Rohdin","Lukáš Burget","Oldřich Plchot","Yanmin Qian","Kai Yu"],"categories":["SEL"],"content":"","date":1561473531,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561473531,"objectID":"9bebd5e034e075bbcdae4bdd320fd095","permalink":"https://wsstriving.github.io/publication/2019-is-phoneme/","publishdate":"2020-06-25T22:38:51+08:00","relpermalink":"/publication/2019-is-phoneme/","section":"publication","summary":"We proposed the segment-level representation for phonetic information and the corresponding segment-level multi-task/adversarial training framework, we revisited the usage the phonetic information for the text-independent embedding learning and designed experiments to verify the assumption: For TI-SV, it could be benificial to remove the phonetic variation in the final speaker embeddings","tags":[],"title":"On the Usage of Phonetic Information for Text-independent Speaker Embedding Extraction","type":"publication"},{"authors":["Shuai Wang","Yanmin Qian and Kai Yu"],"categories":["SEL"],"content":"","date":1498384882,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498384882,"objectID":"49daf81e9b3979471a1960974d3c31d0","permalink":"https://wsstriving.github.io/publication/2017-interspeech/","publishdate":"2020-06-25T18:01:22+08:00","relpermalink":"/publication/2017-interspeech/","section":"publication","summary":"The first attempt to systematically analyze the information encoded in speaker embeddings (prior to x-vector), detailed analysis on x-vectors could be refered to the paper Probing the Information Encoded in X-vectors from JHU ","tags":[],"title":"What Does the Speaker Embedding Encode?","type":"publication"}]